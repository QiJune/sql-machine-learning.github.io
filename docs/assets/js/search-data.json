{
  "0": {
    "id": "0",
    "title": "About Us",
    "content": "About Add info about the product. Add info about the team. Add info about the contact.",
    "url": "/pages/about.html",
    "relUrl": "/pages/about.html"
  },
  "1": {
    "id": "1",
    "title": "Contribute",
    "content": "Contribute Dev, build and run!",
    "url": "/doc_index/contribute.html",
    "relUrl": "/doc_index/contribute.html"
  },
  "2": {
    "id": "2",
    "title": "Design",
    "content": "Design How we design SQLFLow",
    "url": "/doc_index/design.html",
    "relUrl": "/doc_index/design.html"
  },
  "3": {
    "id": "3",
    "title": "Document",
    "content": "Document Documents of SQLFLOW",
    "url": "/doc_index/doc.html",
    "relUrl": "/doc_index/doc.html"
  },
  "4": {
    "id": "4",
    "title": "SQLFlow: Bridging Data and AI",
    "content": "",
    "url": "/",
    "relUrl": "/"
  },
  "5": {
    "id": "5",
    "title": "Proof of Concept: ALPS Submitter",
    "content": "Proof of Concept: ALPS Submitter ALPS (Ant Learning and Prediction Suite) provides a common algorithm-driven framework in Ant Financial, focusing on providing users with an efficient and easy-to-use machine learning programming framework and a financial learning machine learning algorithm solution. This module is used to submit ALPS machine learning training tasks in SQLFlow. Precondition For machine learning models, we only consider TensorFlow premade estimator. To simplify the design, we only execute training without evaluation in the estimator. If a table cell is encoded, we assume the user always provides enough decoding information such as dense/sparse, shape via expression such as DENSE, SPARSE Data Pipeline Standard Select -&gt; Train Input Table -&gt; Decoding -&gt; Input Fn and TrainSpec The standard select query is executed in SQL Engine like ODPS, SparkSQL, we take the result table as the input of training. If a table cell is encoded, we assume the user always provides enough decoding information such as dense/sparse, shape, delimiter via expression such as DENSE, SPARSE. The decode expression must exist in COLUMN block. Dense Format It is the dense type of encoded data if we have multiple numeric features in a single table cell. For example, we have numeric features such as price, count and frequency, splice into a string using a comma delimiter. In this situation, the DENSE expression should be used to declare the decoding information such as the shape, delimiter. DENSE(table_column, shape, dtype, delimiter) Args: table_column: column name shape(int): shape of dense feature delimiter(string): delimiter of encoded feature Sparse Format It is the sparse type of encoded data if we not only have multiple features in a single table cell but also mapping the feature to sparse format. For example, we have features such as city, gender and interest which has multiple values for each feature. The values of city are beijing, hangzhou, chengdu. The values of gender are male and female. The values of interest are book and movie. Each of these values has been mapped to an integer and associate with some group. `beijing` -&gt; group 1, value 1 `hangzhou` -&gt; group 1, value 2 `chengdu` -&gt; group 1, value 3 `male` -&gt; group 1, value 4 `female` -&gt; group 1, value 5 `book` -&gt; group 2, value 1 `movie` -&gt; group 2, value 2 If we use colon as the group/value delimiter and use comma as the feature delimiter, “3:1, 4:1, 2:2” means “chengdu, male, movie”. SPARSE(table_column, shape, dtype, delimiter, group_delimiter) Args: table_column: column name shape(list): list of embedding shape for each group delimiter(string): delimiter of feature group_delimiter(string): delimiter of value and group Decoding The actual decoding action is not happened in this submitter but in ALPS inside. What we should do here is just generate the configuration file of ALPS. Let’s take an example for training a classifier model for credit card fraud case. Table Data column/row c1 c2 c3 r1 10,2,4 3:1,4:1,2:2 0 r2 500,20,10 2:1,5:1,2:1 1 The column c1 is dense encoded and c2 is sparse encoded, c3 is label column. SQL select c1, c2, c3 as class from kaggle_credit_fraud_training_data TRAIN DNNClassifier WITH ... COLUMN DENSE(c1, shape=3, dtype=float, delimiter=&#39;,&#39;) SPARSE(c2, shape=[10, 10], dtype=float, delimiter=&#39;,&#39;, group_delimiter=&#39;:&#39;) LABEL class ALPS Configuration # graph.conf ... schema = [c1, c2, c3] x = [ {feature_name: c1, type: dense, shape:[3], dtype: float, separator:&quot;,&quot;}, {feature_name: c2, type: sparse, shape:[10,10], dtype: float, separator:&quot;,&quot;, group_separator:&quot;:&quot;} ] y = {feature_name: c3, type: dense, shape:[1], dtype: int} ... Feature Pipeline Feature Expr -&gt; Semantic Analyze -&gt; Feature Columns Code Generation -&gt; Estimator Feature Expressions In SQLFlow, we use Feature Expressions to represent the feature engineering process and convert it into the code snippet using TensorFlow Feature Column API. Feature Expressions NUMERIC(key, dtype, shape) BUCKETIZED(source_column, boundaries) CATEGORICAL_IDENTITY(key, num_buckets, default_value) CATEGORICAL_HASH(key, hash_bucket_size, dtype) CATEGORICAL_VOCABULARY_LIST(key, vocabulary_list, dtype, default_value, num_oov_buckets) CATEGORICAL_VOCABULARY_FILE(key, vocabulary_file, vocabulary_size, num_oov_buckets, default_value, dtype) CROSS(keys, hash_bucket_size, hash_key) The feature expressions must exist in COLUMN block. Here is an example which do BUCKETIZED on c2 then CROSS with c1. select c1, c2, c3 as class from kaggle_credit_fraud_training_data TRAIN DNNClassifier WITH ... COLUMN CROSS([NUMERIC(c1), BUCKETIZED(NUMERIC(c2), [0, 10, 100])]) LABEL class Semantic Analyze Feature Expressions except for Tensorflow Feature Column API should raise an error. /* Not supported */ select * from kaggle_credit_fraud_training_data TRAIN DNNClassifier WITH ... COLUMN NUMERIC(f1 * 10) Feature Columns Code Generation We transform feature columns expression to a code snippet and wrap it as a CustomFCBuilder which extends from alps.feature.FeatureColumnsBuilder. Review the above example, the generated code snippet is this: from alps.feature import FeatureColumnsBuilder class CustomFCBuilder(FeatureColumnsBuilder): def build_feature_columns(): fc1 = tf.feature_column.numeric_column(&#39;c1&#39;) fc2 = tf.feature_column.numeric_column(&#39;c2&#39;) fc3 = tf.feature_column.bucketized_column(fc2, boundaries = [0, 10, 100]) fc4 = tf.feature_column.crossed_column([fc2, fc3]) return [fc4] ALPS framework will execute this code snippet and pass the result to the constructor method of the estimator. Parameters We use WITH block to set the parameters of training. If the name is prefixed with estimator, it is the parameter of the constructor method of the Estimator. If the name is prefixed with train_spec, it is the parameter of the constructor method of the TrainSpec. If the name is prefixed with input_fn, it is the parameter of the input_fn. Let’s create a DNNClassifier example, the minimum parameters of the constructor method are hidden_units and feature_columns. select c1, c2, c3 as class from kaggle_credit_fraud_training_data TRAIN DNNClassifier WITH estimator.hidden_units = [10, 20], train_spec.max_steps = 2000, input_fn.batch_size = 512 COLUMN CROSS([NUMERIC(c1), BUCKETIZED(NUMERIC(c2), [0, 10, 100])]) LABEL class ... For now, we will pass the result of snippet code as feature_columns parameters and it will raise an error if the estimator expects it as a different name until AS syntax is supported in SQLFlow. select c1, c2, c3, c4, c5 as class from kaggle_credit_fraud_training_data TRAIN DNNLinearCombinedClassifier WITH linear_feature_columns = [fc1, fc2] dnn_feature_columns = [fc3] ... COLUMN NUMERIC(f1) as fc1, BUCKETIZED(fc1, [0, 10, 100]) as fc2, CROSS([fc1, fc2, f3]) as fc3 LABEL class ...",
    "url": "/sqlflow/doc/alps_submitter.html",
    "relUrl": "/sqlflow/doc/alps_submitter.html"
  },
  "6": {
    "id": "6",
    "title": "Demo: Using SQLFlow from Command Line",
    "content": "Demo: Using SQLFlow from Command Line Besides running SQLFlow from Notebook, we could also run it from command line. If you are using Docker for Linux, please change host.docker.internal:3306 to localhost:3306. docker run -it --rm --net=host sqlflow/sqlflow:latest demo --db_user root --db_password root --db_address host.docker.internal:3306 You should be able to see the following: sqlflow&gt; Training a DNNClassifier and run prediction Step one: Let’s see training data from Iris database: sqlflow&gt; select * from iris.train limit 2; -- +--+-+--+-+-+ | SEPAL LENGTH | SEPAL WIDTH | PETAL LENGTH | PETAL WIDTH | CLASS | +--+-+--+-+-+ | 6.4 | 2.8 | 5.6 | 2.2 | 2 | | 5 | 2.3 | 3.3 | 1 | 1 | +--+-+--+-+-+ Step Two: Train a Tensorflow DNNClassifier from train table: sqlflow&gt; SELECT * FROM iris.train TRAIN DNNClassifier WITH n_classes = 3, hidden_units = [10, 20] COLUMN sepal_length, sepal_width, petal_length, petal_width LABEL class INTO sqlflow_models.my_dnn_model; -- ... Training set accuracy: 0.96721 Done training Step Three: Run prediction from a trained model: sqlflow&gt; SELECT * FROM iris.test predict iris.predict.class USING sqlflow_models.my_dnn_model; Step Four: Checkout the prediction result: sqlflow&gt; select * from iris.predict limit 10; Now you have successfully run a demo in SQLFlow to train the model using DNNClassifier and make a simple prediction. More demos are on the road, please stay tuned.",
    "url": "/sqlflow/doc/demo.html",
    "relUrl": "/sqlflow/doc/demo.html"
  },
  "7": {
    "id": "7",
    "title": "Installation",
    "content": "Installation SQLFlow is currently under active development. For those who are interested in trying it out, we have provided the instructions and demo. Play around with it. Any bug report and issue is welcome. :) Preparation Install Docker Community Edition on your Macbook. Build and run a dockerized MySQL server (In this example, called sqlflowdata) following example/datasets/README.md. Note that there is no need to install a local MySQL server, in which case you will have a port conflict in 3306. Pull the latest SQLFlow Docker image: docker pull sqlflow/sqlflow:latest. Running your first SQLFlow query After data is popularized in MySQL, let’s test the installation from running a query in Jupyter Notebook: Start a Docker container that runs SQLFlow server and Jupyter Notebook. If you are using Docker for Linux, please change host.docker.internal:3306 to localhost:3306. docker run --rm -it -p 8888:8888 sqlflow/sqlflow:latest bash -c &quot;sqlflowserver --db_user root --db_password root --db_address host.docker.internal:3306 &amp; SQLFLOW_SERVER=localhost:50051 jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root&quot; If you are using Docker for Mac, please be aware the option --db_address host.docker.internal:3306 where host.docker.internal translates to the host ip address as recommended here. If you are running MySQL on remote, please be aware that MySQL only allows connections from localhost by default. Fix can be found here. Open a web browser, go to localhost:8888 and paste the token output from Notebook command above. In a Notebook cell, you should be able to test a select statement to fetch 5 records from train table in Iris database. %%sqlflow select * from iris.train limit 5; Feel free to explore more examples at example.ipynb if you are new to Jupyter Notebook.",
    "url": "/sqlflow/doc/installation.html",
    "relUrl": "/sqlflow/doc/installation.html"
  },
  "8": {
    "id": "8",
    "title": "CalciteParser gRPC Server for SQLFlow",
    "content": "CalciteParser gRPC Server for SQLFlow How to Build The following build process doesn’t require us to install Java SDK, Maven, protobuf-compiler, and any dependencies. Instead, it installs all such staff into a Docker image and use the Docker image as the build toolchain. Building in Docker containers standardizes development environments of all contributors, keeps our host computer clean, and works on macOS, Linux, BSD, Windows, and all platforms that have Docker. Build the development Docker image: docker build -t calcite:dev . Or, if it takes too long time for you to build the image, please feel free to use mine: docker pull cxwangyi/calcite:dev docker tag cxwangyi/calcite:dev calcite:dev Generate Java source code from protobuf messages: docker run --rm -it -v $PWD:/work -w /work calcite:dev protoc --java_out=. CalciteParser.proto docker run --rm -it -v $PWD:/work -w /work calcite:dev protoc --grpc-java_out=. CalciteParser.proto Build and generate .class files: docker run --rm -it -v $PWD:/work -w /work calcite:dev javac *.java All, actually, we can do all above in a single command: docker run --rm -it -v $PWD:/work -w /work calcite:dev bash ./build_and_test.bash",
    "url": "/sqlflow/calcite-parser/",
    "relUrl": "/sqlflow/calcite-parser/"
  }
  
}
