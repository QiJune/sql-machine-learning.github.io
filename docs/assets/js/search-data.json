{
  "0": {
    "id": "0",
    "title": "About Us",
    "content": "AboutAdd info about the product.Add info about the team.Add info about the contact.",
    "url": "/pages/about.html",
    "relUrl": "/pages/about.html"
  },
  "1": {
    "id": "1",
    "title": "SQLFlow: Bridging Data and AI",
    "content": "",
    "url": "/",
    "relUrl": "/"
  },
  "2": {
    "id": "2",
    "title": "Build TensorFlow from Source Code using Docker",
    "content": "Build TensorFlow from Source Code using DockerTo contribute to TensorFlow, we need to build TensorFlow from source code.  The official guide is great. However, it interleaves the native building process and that using Docker and makes it confusing because packages needed by the former are not by the latter.  Also, we found some useful tricks to start Docker containers in practices that are not in the official guide.  Hence this document.Build the Pip Package in TensorFlow Development ContainerOn either Mac or Linux, or any other OS, we don’t have to install and configure the building tools; instead, we can use a Docker image where all tools have been installed and properly configured.      Get the Docker image containing all the building tools:    docker pull tensorflow/tensorflow:latest-devel            Then, let’s get the source code. On any OS, please install git using the native package manager. For example, on Ubuntu, please    sudo apt-get install git        or, on Mac,    brew install git        Then, use the git just installed, let’s clone tensorflow source code:    git clone --recursive https://github.com/tensorflow/tensorflowcd tensorflow        By default, we will be on the master branch.  Feel free to do you change in your feature branches, or switch to a release branch, for example:    git checkout v1.11.0git checkout -b v1.11.0            Then, let us start a Docker container running the tensorflow/tensorflow:latest-devel image:    docker run --rm -it     -w /tensorflow     -v $PWD:/tensorflow     -v $HOME/.cache:/root/.cache     -e &quot;HOST_PERMS=$(id -u):$(id -g)&quot;     tensorflow/tensorflow:latest-devel     /bin/bash              -w /tensorflow brings us to the /tensorflow directory in the container once after we start it.      -v $PWD:/tensorflow maps the current directory, which is the just cloned TensorFlow source directory on the host, to /tensorflow in the container.      -v $HOME/.cache:/root/.cache maps the Bazel temporary directory on the host into the container, so the intermediate files generated by Bazel running in the container are actually saved on the host.  This allows us to interrupt the container during the build and restart it later to resume the building.      e &quot;HOST_PERMS=$(id -u):$(id -g)&quot; passes the user identification on the host into the container as an environment variable.  We can reset the mode of files generated in the container to this user identity.            From now on, we will be working in the container.  Let us first configure the building:    ./configure        Usually, I would simply choose all the default options by hitting enter all the way down.        Build the pip package builder:    bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package            Build the pip package and save it into /tensorflow:    ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tensorflow            Change the generated wheel file into the usual mode as on the host:    chown $HOST_PERMS /mnt/tensorflow-*.whl      Install the Pip Pacakge in TensorFlow Development Container      Let’s now try the new pip package. First, we need to uninstall the current tensorflow pip package, then we install the newly built one:    pip uninstall tensorflowpip install /tensorflow/tensorflow-*.whl            Now, we can verify if the new package works.  First, we need to switch to a directory out from /tensorflow, so we don’t import from the source directory:    cd /tmp  # other directories also work        then, we can try import the newly installed tensorflow package and verify it:    python&amp;gt;&amp;gt;&amp;gt; import tensorflow&amp;gt;&amp;gt;&amp;gt; print(tensorflow.__version__)            Now, let us quit from Python and from the Docker container.  We should see the tensorflow-*.whl file on the host in the current directory.  Install the Pip Package in a Clean Python PackageAfter we quit from the development container, we should see the wheel file in the TensorFlow source directory on the host.  Now, we can start a Python container and install the Pip package in it.      Start the Python container    docker run --rm -it -v $PWD:/tensorflow python:2.7 bash            Install the pip pacakge in the container    pip install /tensorflow/tensorflow*.whl            Try TensorFlow by starting Python in the container    python&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf&amp;gt;&amp;gt;&amp;gt; print(tf.__version__)      ",
    "url": "/doc/build-tensorflow.html",
    "relUrl": "/doc/build-tensorflow.html"
  },
  "3": {
    "id": "3",
    "title": "Canonical Development Environment",
    "content": "Canonical Development EnvironmentReferring to this example,we create a canonical development environment for Go and Python programmers using Docker.Editing on HostWhen we use this Docker image for daily development work, the source code relieson the host computer instead of the container. The source code includes this repoand all its dependencies, for example, the Go package google.golang.org/grpc.Code-on-the-host allows us to run our favorite editors (Emacs, VIM, Eclipse, and more)on the host.  Please free to rely on editors add-ons to analyze the source codefor auto-completion.Building in ContainerWe build a Docker image that contains development tools:  The Python interpreter  The Go compiler  The protobuf compiler  The protobuf to Go compiler extension  The protobuf to Python compiler extensionBecause this repo contains Go code, please make sure that you have the directorystructure required by Go. On my laptop computer, I haveexport GOPATH=$HOME/goYou could have your $GOPATH pointing to any directory you like.Given $GOPATH$ set, we could git clone the source code of our project by running:go get github.com/sql-machine-learning/sqlflowChange the directory to our project root, and we can use go get to retrieveand update Go dependencies.cd $GOPATH/src/github.com/sql-machine-learning/sqlflowgo get -u -t ./...Note -t instructs get to also download the packages required to buildthe tests for the specified packages.As all Git users would do, we run git pull from time to time to sync up withothers’ work. If somebody added new dependencies, we might need to run go -u ./...after git pull to update dependencies.To build this project, we need the protobuf compiler, Go compiler, Python interpreter,gRPC extension to the protobuf compiler. To ease the installation and configurationof these tools, we provided a Dockerfile to install them into a Docker image.To build the Docker image:docker build -t sqlflow:dev -f Dockerfile.dev .DevelopmentBuild and TestWe build and test the project inside the docker container.To run the container, we need to map the $GOPATH directory on the host into the/go directory in the container, because the Dockerfile configures /go asthe $GOPATH in the container:docker run --rm -it -v $GOPATH:/go      -w /go/src/github.com/sql-machine-learning/sqlflow      sqlflow:dev bashInside the Docker container, start a MySQL server in the backgroundservice mysql start&amp;amp;run all the tests asgo generate ./...go install ./...go test -v ./...where go generate invokes the protoc command to translate server/sqlflow.protointo server/sqlflow.pb.go and go test -v builds and run unit tests.ReleaseThe above build process currently generates two binary files in$GOPATH/bin on the host.  To package them into a Docker image,please rundocker build -t sqlflow -f ./Dockerfile $GOPATH/binTo publish the released Docker image to our official DockerHubdocker tag sqlflow sqlflow/sqlflow:latestdocker push sqlflow/sqlflow:latestDemo: Command line PromptThe demo requires a MySQL server instance with populated data. If we don’t, we couldfollow example/datasets/README.md to start one on the host.After setting up MySQL, run the following inside the Docker containergo run cmd/demo/demo.go --db_user root --db_password root --db_address host.docker.internal:3306You should be able to see the following promptsqlflow&amp;gt;",
    "url": "/doc/build.html",
    "relUrl": "/doc/build.html"
  },
  "4": {
    "id": "4",
    "title": "Closing the producer goroutine from the consumer",
    "content": "Closing the producer goroutine from the consumerThe producer-and-consumer pattern is well used in Go concurrent programming. Whenthe consumer stops, we want to gracefully stop the producer as well.ProblemWhen a gRPC server receives a streaming request,  it usually calls afunction that returns a channel,reads the result from that channel and send the result to the client one by one.Take the following code for instance: upon receiving a request, the main goroutineService calls launchJob. launchJob starts a separate goroutine as an anonymousfunction call and returns a channel. In the anonymous function, items will be sent tochannel. And Service on the otherside of the channel will reads from it.func Service(req *Request, stream *StreamResponse) error {  result := launchJob(req.Content)  for r := range result {    if e := stream.Send(result); e != nil {      // should we signal the running goroutine so it will stop sending?      return e    }  }}func launchJob(content string) chan Item {  c := make(chan Item)    go func() {    defer close(c)    acquireScarceResources()    defer releaseScarceResources()    ...    // if stream.Send(result) returns an error and the Service returns, this will be blocked    c &amp;lt;- Item{}    ...  }()    return c}There is a major problem in this implementation. As pointed out by the comment,if the Send in Service returns an error, the Service function will return,leaving the anonymous function being blocked on c &amp;lt;- Item{} forever.This problem is important because the leaking goroutine usually owns scarce systemresources such as network connection and memory.Solution: pipeline explicit cancellationInspired by this blog post sectionExplicit cancellation, we can signal the cancellation via closing on a separatechannel. And we can follow the terminology as io.Pipe.package sqlimport (	&quot;errors&quot;)var ErrClosedPipe = errors.New(&quot;pipe: write on closed pipe&quot;)// pipe follows the design at https://blog.golang.org/pipelines// - wrCh: chan for piping data// - done: chan for signaling Close from Reader to Writertype pipe struct {	wrCh chan interface{}	done chan struct{}}// PipeReader reads real datatype PipeReader struct {	p *pipe}// PipeWriter writes real datatype PipeWriter struct {	p *pipe}// Pipe creates a synchronous in-memory pipe.//// It is safe to call Read and Write in parallel with each other or with Close.// Parallel calls to Read and parallel calls to Write are also safe:// the individual calls will be gated sequentially.func Pipe() (*PipeReader, *PipeWriter) {	p := &amp;amp;pipe{		wrCh: make(chan interface{}),		done: make(chan struct{})}	return &amp;amp;PipeReader{p}, &amp;amp;PipeWriter{p}}// Close closes the reader; subsequent writes to thefunc (r *PipeReader) Close() {	close(r.p.done)}// ReadAll returns the data chan. The caller should// use it as `for r := range pr.ReadAll()`func (r *PipeReader) ReadAll() chan interface{} {	return r.p.wrCh}// Close closes the writer; subsequent ReadAll from the// read half of the pipe will return a closed channel.func (w *PipeWriter) Close() {	close(w.p.wrCh)}// Write writes the item to the underlying data stream.// It returns ErrClosedPipe when the data stream is closed.func (w *PipeWriter) Write(item interface{}) error {	select {	case w.p.wrCh &amp;lt;- item:		return nil	case &amp;lt;-w.p.done:		return ErrClosedPipe	}}And the consumer and producer be can implemented asfunc Service(req *Request, stream *StreamResponse) error {  pr := launchJob(req.Content)  defer pr.Close()  for r := range pr.ReadAll() {    if e := stream.Send(r); e != nil {      return e    }  }}func launchJob(content string) PipeReader {	pr, pw := Pipe()	go func() {		defer pw.Close()				if err := pw.Write(Item{}); err != nil {			return		}	}	return pr}Further Reading  Google Form: Channel send timeout  Go by Example: Timeouts  Google I/O 2013 - Advanced Go Concurrency Patterns  Go Concurrency Patterns Talk  Go Concurrency Patterns: Pipelines and cancellation",
    "url": "/doc/close_producer_from_consumer.html",
    "relUrl": "/doc/close_producer_from_consumer.html"
  },
  "5": {
    "id": "5",
    "title": "Compatibility with Various SQL Engines",
    "content": "Compatibility with Various SQL EnginesSQLFlow interacts with SQL engines like MySQL and Hive, while different SQL engines use variants of SQL syntax, it is important for SQLFlow to have an abstraction layer that hides such differences.SQLFlow calls Go’s standard database API. The submitter programs generated by SQLFlow call Python’s database API.  Both APIs abstract the interface to various SQL engines; however, they are insufficient for SQLFlow to work.  In this document, we examine all interactions between SQLFlow and the SQL engine so to identify what SQLFlow authors have to abstract in addition to calling Go’s and Python’s database APIs.Data Operations in GoData RetrievalThe basic idea of SQLFlow is to extend the SELECT statement of SQL to have the TRAIN and PREDICT clauses.  For more discussion, please refer to the syntax design.  SQLFlow translates such “extended SQL statements” into submitter programs, which forward the part from SELECT to TRAIN or PREDICT, which we call the “standard part”, to the SQL engine.  SQLFlow also accepts the SELECT statement without TRAIN or PREDICt clauses and would forward such “standard statements” to the engine.  It is noticeable that the “standard part” or “standard statements” are not standardized.  For example, various engines use different syntax for joining.  MySQL: SELECT pet.name, comment FROM pet, event WHERE pet.name =event.name; with keyword WHERE .  Hive: SELECT pet.name, comment FROM pet JOIN event ON (pet.name =event.name) with keyword JOIN and ON.  ODPS and SQLite use either INNER JOIN or OUTER JOIN.Fortunately, as SQLFlow forwards the above parts to the engine,  it doesn’t have to care much about the differences above.Metadata RetrievalTo verify the semantics of users’ inputs, SQLFlow needs to retrieve the schema of tables.  For example, the input might beSELECT name, age, income FROM employee TRAIN DNNRegressor WITH hidden_layers=[10,50,10] COLUMN name, agee LABEL income;In the above example, the user misspelled the field name age in the COLUMN clause as “agee”. SQLFlow must be able to find that out.To do that, SQLFlow needs to query the field names from the SQL engine.  However, different engines use various syntax. For example:  MySQL: DESCRIBE/DESC employee;  Hive: DESCRIBE FORMATTED employee;  ODPS: DESC employee;  SQLite: PRAGMA table_info([employee]);The returned data format varies too. Our solution to avoid such differences is not-to-use-them; instead, SQLFlow retrieves the table schema by running a query like SELECT * FROM employee LIMIT 1; and inferring field types using the mechanism called DatabaseTypeName provided by SQL engines drivers beneath the Go’s standard database API.Prepare Prediction TableA SQLFlow prediction job writes its prediction results into a table. It prepares the prediction table by  Dropping previous prediction table DROP TABLE IF EXISTS my_table;  Creating table with schema CREATE TABLE my_table (name1, type1, name2 type2);Most SQL engines, including MySQL, Hive, ODPS, SQLite, support both statements.Translate Database Column Type to TensorFlow Feature Column TypeAfter retrieving database column type name through DatabaseTypeName, we can derive TensorFlow’s feature column type via a mapping such as {&quot;FLOAT&quot;, &quot;DOUBLE&quot;} -&amp;gt; tf.numeric_column.Save ModelSQLFlow saves trained ML model by dumping the serialized the model directory into a table. It first creates a table by CREATE TABLE IF NOT EXISTS %s (id INT AUTO_INCREMENT, block BLOB, PRIMARY KEY (id)) and insert blobs by INSERT INTO %s (block) VALUES(?).Note that Hive and ODPS doesn’t have BLOB type, we need to use BINARY (docs at ODPS, Hive) instead.Also, note that Hive and ODPS doesn’t support AUTO_INCREMENT, we need to implemented auto increment logic in sqlfs.Load ModelSQLFlow loads trained ML model by reading rows in a table and deserializing the blob to a model directory.It reads rows by running SELECT block FROM %s ORDER BY id, which is supported by most databases.Data Operations in PythonConnect to SQL EnginesThanks to the Python database API, connecting to different databases follows a similar API.conn = mysql.connector.connect(user='scott', password='password',                               host='127.0.0.1',                               database='employees')conn = sqlite3.connect('path/to/your/sqlite/file')conn = pyhive.connect('localhost')cursor = conn.cursor()cursor.execute('select * from my_table;')Insert Prediction Result into Prediction TablePython database API provides execute_many(sql, value)  to insert multiple values at once. So one can prepare the following insertion statement. Please be aware that MySQL and SQLite use INSERT INTO to insert rows while Hive and ODPS use INSERT INTO TABLE.-- MySQL, SQLiteINSERT INTO table_name VALUES (value1, value2, value3, ...);-- Hive, ODPSINSERT INTO TABLE table_name VALUES (value1, value2, value3, ...);",
    "url": "/doc/database_abstraction_layer.html",
    "relUrl": "/doc/database_abstraction_layer.html"
  },
  "6": {
    "id": "6",
    "title": "Run MySQL Server and Client in Docker Containers",
    "content": "Run MySQL Server and Client in Docker ContainersThe document explains how to setup MySQL in our development environment.Run MySQL Server in a Docker Containerdocker run --rm     -v /tmp/test1:/var/lib/mysql     --name mysql01     -e MYSQL_ROOT_PASSWORD=root     -e MYSQL_ROOT_HOST='%'     -p 3306:3306     -d     mysql/mysql-server:8.0      the -v option ensures that the database is saved on the host.  The default directory where MySQL saves the database is /var/lib/mysql. This directory can be configured in /etc/mysql/my.cnf, as explained in this post.  By overlaying the directory /tmp/test1 on the host to /var/lib/mysql, we “cheat” MySQL to save databases on the host.  So, we can kill the container and restart it, and the database is still there.    Please be aware that the directory on the host must be empty the first time we run the above command; otherwise, MySQL would fail to initialize.  I figured out this problem after several failures using docker logs.        the -e option sets the root password of MySQL to “root”.  Feel free to set it to any password you like.        the second -e options sets MYSQL_ROOT_HOST to a wildcard so to allow clients connecting to the server via TCP/IP as the user “root”.  This trick works with MySQL 5.7 and 8.0, but not the most recent under-development version.        the --name option names the container to mysql01, which can be used to refer to this container.        the -p option maps the port 3306, on which the MySQL server listens, to the same port on the host, so that clients could connect to the server via TCP/IP.  Run MySQL Client in the Server Containerdocker exec -it mysql01 mysql -uroot -pThis command executes the command mysql, which is the command line tool of MySQL, in the container named mysql01.      The command line flags of mysql include -u, which specifies the username of MySQL, and -p, which makes MySQL prompts for the password.  For this example, we should type the password “root”, which was set in the previous command.        Please wait for a few seconds after the starting of the MySQL server container before we execute the client; otherwise, the startup of the client might fail.        Once we get into the MySQL client, we can type SQL commands, e.g.,    show databases;create database yi;      Run Client in a Different Container on the Same Hostdocker run --rm -it     -v /tmp/test1:/var/lib/mysql     mysql/mysql-server:8.0     mysql -uroot -p  The -v option maps the database directory on the host to the client container. This mapping is necessary because, by default, the client talks to the server via Unix socket /var/lib/mysql/mysql.sock, which is /tmp/test1/mysql.sock on the host.Run Client in a Container on a Remote Hostdocker run     --rm -it     mysql/mysql-server:8.0     mysql -h 192.168.1.3 -P 3306 -uroot -p  the -h option tells the client where the server is running on.  In this example, the given IP is the one of the host where I ran the MySQL server container.Please be aware that the above command works only if the server allows remote connections.Run Python Client in a ContainerTo write a Python client, we need to install the Python package mysql-connector-python.FROM python:2.7RUN pip install mysql-connector-pythonPlease be aware that some documents says that we need to install mysql-connector.  I tried; but the mysql.connector.connect call failed with the error mysql.connector.errors.NotSupportedError: Authentication plugin 'caching_sha2_password' is not supported.Build the Docker image:docker build -t sqlflow .Run the image:docker run --rm -it sqlflow bashand we can start Python and run the following Python code snippet&amp;gt;&amp;gt;&amp;gt; import mysql.connector&amp;gt;&amp;gt;&amp;gt; db = mysql.connector.connect(user='root', passwd='root', host='192.168.1.3')&amp;gt;&amp;gt;&amp;gt; print(db)&amp;lt;mysql.connector.connection_cext.CMySQLConnection object at 0x7fbab9f3fed0&amp;gt;Run a Go ClientIn order to connect to a database, you need to import the database’s driver first.export GOPATH=$HOME/gogo get -u github.com/go-sql-driver/mysqlgo run the following filepackage mainimport (	&quot;database/sql&quot;	&quot;github.com/go-sql-driver/mysql&quot;	&quot;log&quot;)func main() {	testConfig := &amp;amp;mysql.Config{		User:   &quot;root&quot;,		Passwd: &quot;root&quot;,		Net:    &quot;tcp&quot;,		Addr:   &quot;localhost:3306&quot;,	}	db, e := sql.Open(&quot;mysql&quot;, testConfig.FormatDSN())	if e != nil {		log.Fatal(e)	}	defer db.Close()}",
    "url": "/doc/mysql-setup.html",
    "relUrl": "/doc/mysql-setup.html"
  },
  "7": {
    "id": "7",
    "title": "Piping Responses",
    "content": "Piping ResponsesStreaming ResponsesAs described in the overall design, a SQLFlow job could be a standard or an extended SQL statemnt, where an extended SQL statement will be translated into a Python program.  Therefore, each job might generate up to the following data streams:  standard output, where each element is a line of text,  standard error, where each element is a line of text,  data rows, where the first element consists of fields name/types, and each of the rest is a row of data,  status, where the element could be pending, failed, and succeeded.To create good user experience, we need to pipe these responses from SQLFlow jobs to Jupyter Notebook in real-time.Stages in the PipeThe pipe that streams outputs from SQLFlow jobs to the Jupyter Notebook consists of the following stages:Web browser  ↑ | HTTP ↓Jupyter Notebook server ↑ | ZeroMQ streams: Shell, IOPub, stdin, Controls, Heartbeat ↓iPython kernel ↑ | IPython magic command framework ↓SQLFlow magic command for Jupyter ↑ | gRPC ↓SQLFlow server ↑ | Go channels ↓SQLFlow job manager (Go functions) ↑ | IPC with Go's standard library ↓ SQLFlow jobsIn the above figure, from the SQLFlow magic command to the bottom layer are our work.StreamingWe have two alternative ideas: multiple streams and a multiplexing stream.We decided to use a multiplexing stream because we had a unsuccessful trial with the multiple streams idea: we make the job writes to various Go channels and forward each Go channel to a streaming gRPC call, as the following:Multiple streamsThe above figure shows that there are multiple streams between the Jupyter Notebook server and Jupyter kernels.  According to the document, there are five: Shell, IOPub, stdin, Control, and Heartbeat.  These streams are ZeroMQ streams.  We don’t use ZeroMQ, but we can take the idea of having multiple parallel streams in the pipe.service SQLFlow {    rpc File(string sql) returns (int id) {}    rpc ReadStdout(int id) returns (stream string) {}    rpc ReadStderr(int id) returns (stream string) {}    rpc ReadData(int id) returns (stream Row) {}    rpc ReadStatus(int id) returns (stream int) {}}However, we realized that if the user doesn’t call any one of the SQLFlow.Read... call, there would be no forwarding from the Go channel to Jupyter, thus the job would block forever at writing.A Multiplexing StreamAnother idea is multiplexing all streams into one. For example, we can have only one ZeroMQ stream, where each element is a polymorphic type – could be a text string or a data row.service SQLFlow {    rpc Run(string sql) returns (stream Response) {}}// Only one of the following fields should be set.message Response {    oneof record {        repeated string head = 1;             // Column names.        repeated google.protobuf.Any row = 2; // Cells in a row.        string log = 3;                       // A line from stderr or stdout.    }}",
    "url": "/doc/pipe.html",
    "relUrl": "/doc/pipe.html"
  },
  "8": {
    "id": "8",
    "title": "Quick start",
    "content": "Quick startSQLFlow is currently under active development. For those who are interested in tryingit out, we have provided several demos. Play around with it. Any bug report andissue are welcomed. :)Setup  Install Docker.  Set up a MySQL server following example/datasets/README.md.  Pull the latest SQLFlow Docker image: docker pull sqlflow/sqlflow:latest.Demo 1: Jupyter Notebook  Start a Docker container that runs sqlflowserver and Jupyter Notebook    docker run --rm -it -p 8888:8888 sqlflow/sqlflow:latest  bash -c &quot;sqlflowserver --db_user root --db_password root --db_address host.docker.internal:3306 &amp;amp;SQLFLOW_SERVER=localhost:50051 jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root&quot;        If you are running MySQL on the localhost and you are using Docker for Mac, pleasebe aware the option --db_address host.docker.internal:3306 wherehost.docker.internal translates to the host ip address as recommended here.    If you are running MySQL on remote, please be aware that MySQL only allows connections from localhostby default. Fix can be found here.        Open a Web browser and direct to localhost:8888 and input the token. Then youcan create notebooks. In a cell, you should be able to type    %%sqlflowselect 1        Explore more examples at example.ipynbDemo 2: Command Line Prompt  Start a Docker container that runs SQLFlow command line prompt.docker run -it --rm --net=host sqlflow/sqlflow:latest demo  --db_user root --db_password root --db_address host.docker.internal:3306You should be able to see the following prompt.sqlflow&amp;gt;Example  Select data    sqlflow&amp;gt; select * from iris.train limit 2;-----------------------------+--------------+-------------+--------------+-------------+-------+| SEPAL LENGTH | SEPAL WIDTH | PETAL LENGTH | PETAL WIDTH | CLASS |+--------------+-------------+--------------+-------------+-------+|          6.4 |         2.8 |          5.6 |         2.2 |     2 ||            5 |         2.3 |          3.3 |           1 |     1 |+--------------+-------------+--------------+-------------+-------+        Train a Tensorflow DNNClassifier    sqlflow&amp;gt; SELECT *FROM iris.trainTRAIN DNNClassifierWITH n_classes = 3, hidden_units = [10, 20]COLUMN sepal_length, sepal_width, petal_length, petal_widthLABEL classINTO sqlflow_models.my_dnn_model;-----------------------------...Training set accuracy: 0.96721Done training        Prediction using a trained model    sqlflow&amp;gt; SELECT *FROM iris.testpredict iris.predict.classUSING sqlflow_models.my_dnn_model;        Checkout prediction result    sqlflow&amp;gt; select * from iris.predict limit 10;      ",
    "url": "/doc/quickstart.html",
    "relUrl": "/doc/quickstart.html"
  },
  "9": {
    "id": "9",
    "title": "Extended SQL Parser Design",
    "content": "Extended SQL Parser DesignThis documentation explains the technical decision made in building a SQLparser in Go. It is used to parsed the extended SELECT syntax of SQL thatintegrates TensorFlow Estimators.Related WorkLexer and Parser GeneratorIn 2001, when I was in graduate school, I defined an extended SQL syntax forquerying distributed relational databases, as part of the course project ofDistributed RDBMS by Prof. Li-zhu Zhou.  I wrote the parser using bison (amodern version of yacc) and flex (a modern version oflex).  yacc and lex generate C code;bison and flex generate C++ code. However, this time, I’d use Go.I surveyed goyacc, astandard Go tool.  The usage is very similar to that of yacc and bison.However, the Go toolchain doesn’t provide a tool like lex/flex.Google revealed golex, which is out ofmaintenance.The Mediumpostrecommends Ragel, which is a C++program and could generate Go lexer; however, it lacks documents.Handwritten Lexer and ParserSome documents, including thisonerecommends handwriting lexers.  However, it doesn’t explain how to write theparser.GoAcademy always provides high-quality tech blog posts.  Thisone is from theauthor of InfluxDB.  However, Istopped at where it explains wrapping a SQL statement as a string by anio.Reader, because it is obvious that we should keep the string as a string sothat that token strings could refer to the same memory storage of the SQLstatement.Following a link in the above GoAcademy post, I found Rob Pike’s excellent talkon how to write a lexer in Go in 2011.  Many works after that change Rob’simplementation somehow but always lead to longer and less comprehensiblecodebases.The ChoiceTherefore, I wrote the lexer and parser both following Rob Pike’s idea. Afterfew days work, I realized that:  I should borrow the idea from Rob to represent SQL statements as strings, butnot io.Reader as other work do,  but no need to use channels and goroutines at all, and  it is technically intractable to write a SQL lexer/parser manually.So, I switched to write a lexer manually, and to generate the parser usinggoyacc.  During my work, I referred to thisexample andthe official yaccmanual for detailsabout operator association and precedence.",
    "url": "/doc/sql_parser.html",
    "relUrl": "/doc/sql_parser.html"
  },
  "10": {
    "id": "10",
    "title": "Submitter",
    "content": "SubmitterA submitter is a pluggable module in SQLFlow that is used to submit an ML job to a third party computation service.WorkflowWhen a user types in an extended SQL statement, SQLFlow first parses and semantically verifies the statement. Then SQLFlow either runs the ML job locally or submits the ML job to a third party computation service.In the latter case, SQLFlow produces a job description (TrainDescription or PredictDescription) and hands it over to the submitter. For a training SQL, SQLFlow produces TrainDescription; for prediction SQL, SQLFlow produces PredDescription. The concrete definition of the description looks like the followingtype ColumnType struct {    Name             string // e.g. sepal_length    DatabaseTypeName string // e.g. FLOAT}// SELECT *// FROM iris.train// TRAIN DNNClassifier// WITH//   n_classes = 3,//   hidden_units = [10, 20]// COLUMN sepal_length, sepal_width, petal_length, petal_width// LABEL class// INTO sqlflow_models.my_dnn_model;{% raw %}type TrainDescription struct {    StandardSelect string       // e.g. SELECT * FROM iris.train    Estimator      string       // e.g. DNNClassifier    Attrs          map[string]string // e.g. {{&quot;n_classes&quot;, &quot;3&quot;}, {&quot;hidden_units&quot;, &quot;[10, 20]&quot;}}    X              []ColumnType // e.g. {{&quot;sepal_length&quot;, &quot;FLOAT&quot;}, ...}    Y              ColumnType   // e.g. {&quot;class&quot;, &quot;INT&quot;}    ModelName      string       // e.g. my_dnn_model}{% endraw %}// SELECT *// FROM iris.test// PREDICT iris.predict.class// USING sqlflow_models.my_dnn_model;type PredDescription struct {    StandardSelect string // e.g. SELECT * FROM iris.test    TableName      string // e.g. iris.predict    ModelName      string // e.g. my_dnn_model}Submitter InterfaceThe submitter interface should provide two functions Train and Predict. The detailed definition can be the followingtype Submitter interface {    // Train executes a ML training job and streams job's response through writer.    // A typical Train function should include    // - Loading the training data    // - Initializing the model    // - model.train    // - Saving the trained model to a persistent storage    Train(desc TrainDescription, writer PipeWriter) error    // Predict executes a ML predicting job and streams job's response through writer    // A typical Predict function should include    // - Loading the model from a persistent storage    // - Loading the prediction data    // - model.predict    // - Writing the prediction result to a table    Predict(desc PredictDescription, writer PipeWriter) error}Register a submitterA new submitter can be added asimport (    &quot;.../my_submitter&quot;    &quot;.../sqlflow/sql&quot;)func main() {    // ...    sql.Register(my_submitter.NewSubmitter())    // ...    for {    	sql := recv()    	sql.Run(sql)    }}where sql.Register will put my_submitter instance to package level registry. During sql.Run, it will check whether there is a submitter registered. If there is, sql.Run will run either submitter.Train or submitter.Predict.",
    "url": "/doc/submitter.html",
    "relUrl": "/doc/submitter.html"
  },
  "11": {
    "id": "11",
    "title": "SQLFlow: Design Doc",
    "content": "SQLFlow: Design DocWhat is SQLFlowSQLFlow is a bridge that connects a SQL engine, for example, MySQL, Hive, SparkSQL, Oracle, or SQL Server, and TensorFlow and other machine learning toolkits.  SQLFlow extends the SQL syntax to enable model training and inference.Related WorkWe could write simple machine learning prediction (or scoring) algorithms in SQL using operators like DOT_PRODUCT.  However, this requires copy-n-pasting model parameters from the training program into SQL statements.Some proprietary SQL engines provide extensions to support machine learning.Microsoft SQL ServerMicrosoft SQL Server has the machine learning service that runs machine learning programs in R or Python as an external script:CREATE PROCEDURE generate_linear_modelASBEGIN    EXEC sp_execute_external_script    @language = N'R'    , @script = N'lrmodel &amp;lt;- rxLinMod(formula = distance ~ speed, data = CarsData);        trained_model &amp;lt;- data.frame(payload = as.raw(serialize(lrmodel, connection=NULL)));'    , @input_data_1 = N'SELECT [speed], [distance] FROM CarSpeed'    , @input_data_1_name = N'CarsData'    , @output_data_1_name = N'trained_model'    WITH RESULT SETS ((model varbinary(max)));END;A challenge to the users is that they need to know not only SQL but also R or Python, and they must be capable of writing machine learning programs in R or Python.Teradata SQL for DLTeradata also provides a RESTful service, which is callable from the extended SQL SELECT syntax.SELECT * FROM deep_learning_scorer(  ON (SELECT * FROM cc_data LIMIT 100)  URL('http://localhost:8000/api/v1/request')  ModelName('cc')  ModelVersion('1')  RequestType('predict')  columns('v1', 'v2', ..., 'amount'))The above syntax couples the deployment of the service (the URL in the above SQL statement) with the algorithm.Google BigQueryGoogle BigQuery enables machine learning in SQL by introducing the CREATE MODEL statement.CREATE MODEL dataset.model_name  OPTIONS(model_type='linear_reg', input_label_cols=['input_label'])AS SELECT * FROM input_table;Currently, BigQuery only supports two simple models: linear regression and logistic regression.Design GoalNone of the above meets our requirement.First of all, we want to build an open source software.  Also, we want it to be extensible:      We want it extensible to many SQL engines, instead of targeting any one of them.  Therefore, we don’t want to build our syntax extension on top of user-defined functions (UDFs); otherwise, we’d have to implement them for each SQL engine.        We want the system extensible to support sophisticated machine learning models and toolkits, including TensorFlow for deep learning and xgboost for trees.  Another challenge is that we want SQLFlow to be flexible enough to configure and run cutting-edge algorithms, including specifying feature crosses. At the same time, we want SQLFlow easy to learn – at least, no Python or R code embedded in the SQL statements, and integrate hyperparameter estimation.We understand that a key to address the above challenges is the syntax of the SQL extension. To craft a highly-effective and easy-to-learn syntax, we need user feedback and fast iteration.  Therefore, we’d start from a prototype that supports only MySQL and TensorFlow.  We plan to support more SQL engines and machine learning toolkits later.Design DecisionsAs the beginning of the iteration, we propose an extension to the SQL SELECT statement. We are not going a new statement way like that BigQuery provides CREATE MODEL, because we want to maintain a loose couple between our system and the underlying SQL engine, and we cannot create the new data type for the SQL engine, like CREATE MODEL requires.We highly appreciate the work of TensorFlow Estimator, a high-level API for deep learning. The basic idea behind Estimator is to implement each deep learning model, and related training/testing/evaluating algorithms as a Python class derived from tf.estimator.Estimator.  As we want to keep our SQL syntax simple, we would make the system extensible by calling estimators contributed by machine learning experts and written in Python.The SQL syntax must allow users to set Estimator attributes (parameters of the Python class’ constructor, and those of train, evaluate, or predict).  Users can choose to use default values.  We have a plan to integrate our hyperparameter estimation research into the system to optimize the default values.Though estimators derived from tf.estimator.Estimator run algorithms as TensorFlow graphs; SQLFlow doesn’t restrict that the underlying machine learning toolkit has to be TensorFlow.  Indeed, as long as an estimator provides methods of train, evaluate, and predict, SQLFlow doesn’t care if it calls TensorFlow or xgboost. Precisely, what SQLFlow expect is an interface like the following:class AnEstimatorClass:  __init__(self, **kwargs)  train(self, **kwargs)  evaluate(self, **kwargs)  predict(self, **kwargs)We also want to reuse the feature columns API from Estimator, which allows users to columns of tables in a SQL engine to features to the model.Extended SQL SyntaxAgain, just as the beginning of the iteration, we propose the syntax for training asSELECT * FROM kaggle_credit_fraud_training_dataLIMIT 1000TRAIN DNNClassifier       /* a pre-defined TensorFlow estimator, tf.estimator.DNNClassifier */WITH layers=[100, 200],   /* a parameter of the Estimator class constructor */     train.batch_size = 8 /* a parameter of the Estimator.train method */COLUMN *,                 /* all columns as raw features */       cross(v1, v9, v28) /* plus a derived (crossed) column */LABEL classINTO sqlflow_models.my_model_table;      /* saves trained model parameters and features into a table */We see the redundancy of * in two clauses: SELECT and COLUMN.  The following alternative can avoid the redundancy, but cannot specify the label.SELECT *                  /* raw features or the label? */       corss(v1, v9, v28) /* derived featuers */FROM kaggle_credit_fraud_training_dataPlease be aware that we save the trained models into tables, instead of a variable maintained by the underlying SQL engine.  To invent a new variable type to hold trained models, we’d make our system tightly integrated with the SQL engine, and harms the extensibility to other engines.The result table should include the following information:  The estimator name, e.g., DNNClassifier in this case.  Estimator attributes, e.g., layer and train.batch_size.  The feature mapping, e.g., * and cross(v1, v9, v28).Similarly, to infer the class (fraud or regular), we couldSELECT * FROM kaggle_credit_fraud_development_dataPREDICT kaggle_credit_fraud_development_data.classUSING sqlflow_models.my_model_table;System ArchitectureA Conceptual OverviewIn the prototype, we use the following architecture:SQL statement -&amp;gt; our SQL parser --standard SQL-&amp;gt; MySQL                                 -extended SQL-&amp;gt; code generator -&amp;gt; execution engineIn the prototype, the code generator generates a Python program that trains or predicts.  In either case,  it retrieves the data from MySQL via MySQL Connector Python API,  optionally, retrieves the model from MySQL,  trains the model or predicts using the trained model by calling the user specified  TensorFlow estimator,  and writes the trained model or prediction results into a table.Working with Jupyter Notebook and KubernetesThe following figures shows the system components and their runtime environment.  The left part shows how to run the system on a PC/laptop, the right part shows how to run it on a Kubernetes cluster.",
    "url": "/doc/syntax.html",
    "relUrl": "/doc/syntax.html"
  },
  "12": {
    "id": "12",
    "title": "SQLFlow: Code Walkthrough",
    "content": "SQLFlow: Code WalkthroughUser ExperienceSQLFlow allows users to write SQL programs with extended syntax in Jupyter Notebook or a command-line tool.The following SQL statements train a TensorFlow model named DNNClassifier, which is a Python class derived from tf.estimator.Estimator:SELECT * FROM a_table TRAIN DNNClassifier WITH learning_rate=0.01 INTO sqlflow_models.my_model;And the following statement uses the trained model for prediction.SELECT * FROM b_table PREDICT b_table.predicted_label USING sqlflow_models.my_model;Please be aware that the part in the above statements before the extended keyword TRAIN and PREDICT is a standard SQL statement. This feature simplifies the implementation of the SQLFlow system.System ImplementationIf a SQL statement is of the standard syntax, SQLFlow throws it to the SQL engine and redirects the output to the user; otherwise, SQLFlow translates the statement of extended syntax into a Python program.  Currently, it generates a program that throws the standard-syntax part of SELECT to MySQL, reads the results in the train-loop of a TensorFlow program.  We will talk about how to extend SQLFlow to connect more SQL engines like Oracle, Hive, and SparkSQL, and generates more types of machine learning programs that calls distributed TensorFlow, PyTorch, and xgboost later. Before that, let us explain the system components.SQLFlow as a gRPC ServerSQLFlow is a gRPC server, which can connect with multiple clients.  A typical client is pysqlflow, the SQLFlow plugin for Jupyter Notebook server.  Another once is a text-based client /cmd/sqlflowserver/main.go.Jupyter Notebook          ---(SQL statements)--&amp;gt;       SQLFlow gRPC server(SQLFlow magic command)   &amp;lt;--(a stream of messages)--The protobuf definition of the gRPC service is at /server/proto/sqlflow.proto.  The return of the method SQLFlow.Run is a stream of Reponses, where each represents either a table header, a row, or a log message.  The header and rows are usually from a standard SQL statement, for example, SELECT or DESCRIBE, and the log messages are usually from the running of a generated Python program.SQLFlow in the gRPC ServerOnce the SQLFlow server receives a batch of SQL statements via a gRPC call, it runs the following steps for each statement:  the parser to generate parsing result,  the verifier to verify the semantics given the parsing result,  the code generator to generate a Python program, or the submitter, from the parsing result,  the executor that runs the submitter locally.Step 3. and 4. are only for a SQL statement of extended syntax; otherwise, SQLFlow server proxies the standard-syntax statement to the SQL engine.The executor calls Go’s standard package that captures the stdout and stderr from the submitter process and passing the result back to the gRPC client.  Therefore, it is the responsibility of the submitter to print log messages to its stderr and stdout.Minimal Viable ProductIn the minimal viable product (MVP) of SQLFlow, the code generator generates a Python program consists of two parts:  throw the standard SELECT part in the extended-syntax statement to MySQL via ODBC, and  a loop that reads outputs from the run of the SELECT statement and trains the model (or, using a trained model to predict).The training part calls TensorFlow to update the parameters of the model specified in the TRAIN clause.ExtensibilityBy writing more code generators, we could extend SQLFlow to support more SQL engines, e.g., Hive and Oracle, and use machine learning toolkits, e.g., PyTorch and xgboost, in addition to TensorFlow, on various computing platforms.  You are welcome to add more code generators such as  codegen_distributed_tf.go to generate a submitter program similar to the MVP but runs a distributed TensorFlow training job.  codegen_kubernetes_tf.go to launch a distributed TensorFlow job on a Kubernetes cluster, other than running locally, in the same container as where SQLFlow gRPC server resides.  codegen_gcloud_pytorch.go to launch a submitter that calls PyTorch instead of TensorFlow for training on the Google Cloud.Job ManagementThe current design of the gRPC interface assumes that the connection between the client, e.g., the Jupyter Notebook, and the SQLFlow server keeps alive during the running of the training program.  This assumption is reasonable because even if the user closes her/his Web browser and disconnect to the Jupyter Notebook server, the connection between Jupyter to SQLFlow server might keep alive.  However, this might not be robust enough if the Jupyter Notebook server runs on a user’s laptop and gets killed.  In such a case, the gRPC server cannot stream the messages back to the client and would cause the failure of the submitter.A solution is to change the gRPC interface of SQLFlow server to have a method that files a job and returns immediately, and another method to get a batch of recent messages given a job ID.  We will make a design for that soon.",
    "url": "/doc/walkthrough.html",
    "relUrl": "/doc/walkthrough.html"
  }
}
